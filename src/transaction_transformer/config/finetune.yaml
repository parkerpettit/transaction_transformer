model:
  mode: "finetune"
  row_types: 1
  freeze_embedding: false
  pretrain_checkpoint_dir: "data/models/pretrained"
  finetune_checkpoint_dir: "data/models/finetuned"
  log_dir: "logs"
  
  field_transformer:
    d_model: 72
    n_heads: 8
    depth: 1
    ffn_mult: 4
    dropout: 0.1
    layer_norm_eps: 1.0e-6
    norm_first: true
  
  sequence_transformer:
    d_model: 1080
    n_heads: 12
    depth: 12
    ffn_mult: 4
    dropout: 0.1
    layer_norm_eps: 1.0e-6
    norm_first: true
  
  classification:
    hidden_dim: 512
    depth: 2  # Number of hidden layers in the MLP. 0 is a linear layer
    dropout: 0.1
    output_dim: 1

  embedding:
    emb_dim: 72
    dropout: 0.1
    padding_idx: 0
    freq_encoding_L: 8
    mask_token_init_std: 0.02
  
  training:
    total_epochs: 1
    batch_size: 128 
    learning_rate: 1.0e-4
    model_type: "ar"  # "mlm" or "ar"
    optimizer: "adamw"
    scheduler: "cosine"
    mixed_precision: false
    early_stopping_patience: 5
    device: "cuda"
    num_workers: 4
    positive_weight: 9.08  # Weight for positive (fraud) class. if 1.0, calculate from data
    # Finetune startup / resume control
    resume: false
    resume_path: data/models/finetuned/finetune_best_model.pt # change this if resuming
    from_scratch: false
    pretrained_backbone_path: data/models/pretrained/mlm_pretrain_best_model.pt
    #   Label statistics from full_processed.pt with include_all_fraud: true
    #   Positive samples: 20677
    #   Negative samples: 1704227
    #   Positive ratio: 0.0120
    #   Recommended weights:
    #   Inverse frequency: 82.42
    #   Sqrt inverse frequency: 9.08
    # Recommended: Calculate as negative_samples / positive_samples from your dataset
    # Or use sqrt(negative_samples / positive_samples) for less aggressive weighting
  
  data:
    preprocessed_path: "data/processed/full_processed.pt"
    window: 10
    stride: 10
    num_bins: 100
    group_by: "User"
    include_all_fraud: true
    padding_idx: 0
    mask_idx: 1
    unk_idx: 2
    ignore_idx: -100

metrics:
  run_name: finetune
  wandb: true
  wandb_project: "fraud-detection"
  seed: 42
  deterministic: false
  log_gradients: false
  log_parameters: false
