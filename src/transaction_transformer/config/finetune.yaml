model:
  model_type: "fraud_detection"
  row_types: 1
  freeze_embedding: false
  emb_dropout: 0.1
  clf_dropout: 0.1
  checkpoint_dir: "data/models"
  log_dir: "logs"
  
  field_transformer:
    d_model: 72
    n_heads: 8
    depth: 2
    ffn_mult: 4
    dropout: 0.1
    layer_norm_eps: 1e-6
    norm_first: true
    is_causal: false
  
  sequence_transformer:
    d_model: 256
    n_heads: 8
    depth: 6
    ffn_mult: 4
    dropout: 0.1
    layer_norm_eps: 1e-6
    norm_first: true
    is_causal: false  # Bidirectional for fraud detection
  
  embedding:
    emb_dim: 72
    dropout: 0.1
    padding_idx: 0
    freq_encoding_L: 8
    mask_token_init_std: 0.02
  
  training:
    total_epochs: 5
    batch_size: 32
    learning_rate: 5e-5  # Lower learning rate for finetuning
    weight_decay: 0.01
    warmup_steps: 100
    max_grad_norm: 1.0
    mode: "ar"  # Not used in finetuning but required
    p_field: 0.15
    p_row: 0.10
    joint_timestamp_masking: true
    optimizer: "adamw"
    scheduler: "cosine"
    mixed_precision: false
    gradient_accumulation_steps: 1
    save_steps: 500
    eval_steps: 100
    logging_steps: 50
    save_best_only: true
    early_stopping_patience: 3
    device: "auto"
    num_workers: 4
  
  data:
    data_dir: "data"
    preprocessed_path: "src/transaction_transformer/data/preprocessing/legit_processed.pt"
    window: 10
    stride: 5
    num_bins: 100
    group_by: "User"
    include_all_fraud: true  # Include fraud labels for finetuning
    padding_idx: 0
    mask_idx: 1
    unk_idx: 2

metrics:
  experiment_name: "feature_prediction_finetune"
  run_name: null
  tensorboard: true
  wandb: false
  wandb_project: null
  wandb_entity: null
  wandb_group: null
  seed: 42
  deterministic: false
  log_gradients: false
  log_parameters: false