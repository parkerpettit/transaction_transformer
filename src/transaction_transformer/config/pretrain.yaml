model:
  mode: "pretrain"
  row_types: 1
  freeze_embedding: false
  pretrain_checkpoint_dir: "data/models/pretrained"
  finetune_checkpoint_dir: "data/models/finetuned"
  log_dir: "logs"
  
  field_transformer:
    d_model: 64
    n_heads: 8
    depth: 1
    ffn_mult: 1
    dropout: 0.1
    layer_norm_eps: 1.0e-6
    norm_first: true
  
  sequence_transformer:
    d_model: 64
    n_heads: 8
    depth: 2
    ffn_mult: 1
    dropout: 0.1
    layer_norm_eps: 1.0e-6
    norm_first: true
  
  embedding:
    emb_dim: 64
    dropout: 0.1
    padding_idx: 0
    freq_encoding_L: 8

  head:
    hidden_dim: 64
    depth: 0 # 0 is a linear layer
    dropout: 0.1 # ignored if depth == 0

  
  training:
    total_epochs: 3
    batch_size: 256
    learning_rate: 1.0e-4
    model_type: "mlm"  # "mlm" or "ar"
    p_field: 0.15
    p_row: 0.10
    joint_timestamp_masking: true
    optimizer: "adamw"
    scheduler: "cosine"
    early_stopping_patience: 5
    max_batches_per_epoch: 50  # null for full epoch; integer for debug cap
    num_workers: 4
    resume: false
    resume_path: null
    use_amp: true
  data:
    preprocessed_path: "data/processed/"
    use_local_inputs: false  # if true, skip artifact download and use local files
    raw_csv_path: "data/raw/card_transaction.v1.csv"
    raw_artifact_name: "raw-card-transactions"
    preprocessed_artifact_name: "preprocessed-card"
    window: 10
    stride: 5
    num_bins: 100
    group_by: "User"
    include_all_fraud: false  # ignored for pretrain; pretrain always uses LEGIT splits
    padding_idx: 0
    mask_idx: 1
    unk_idx: 2
    ignore_idx: -100

metrics:
  run_name: debug-pretrain
  wandb_project: "transaction-transformer"

