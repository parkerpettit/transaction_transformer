model:
  mode: "pretrain"
  row_types: 1
  freeze_embedding: false
  pretrain_checkpoint_dir: "data/models/pretrained"
  finetune_checkpoint_dir: "data/models/finetuned"
  log_dir: "logs"
  
  field_transformer:
    d_model: 72
    n_heads: 8
    depth: 1
    ffn_mult: 4
    dropout: 0.1
    layer_norm_eps: 1.0e-6
    norm_first: true
  
  sequence_transformer:
    d_model: 1080
    n_heads: 12
    depth: 12
    ffn_mult: 4
    dropout: 0.1
    layer_norm_eps: 1.0e-6
    norm_first: true
  
  embedding:
    emb_dim: 72
    dropout: 0.1
    padding_idx: 0
    freq_encoding_L: 8
    mask_token_init_std: 0.02

  head:
    hidden_dim: 512
    depth: 0
    dropout: 0.1

  
  training:
    total_epochs: 1
    batch_size: 64
    learning_rate: 1.0e-4
    model_type: "mlm"  # "mlm" or "ar"
    p_field: 0.15
    p_row: 0.10
    joint_timestamp_masking: true
    optimizer: "adamw"
    scheduler: "cosine"
    mixed_precision: false
    early_stopping_patience: 5
    device: "cuda"
    num_workers: 4
    # Resume control (pretraining)
    resume: false
    resume_path: data/models/pretrained/mlm_pretrain_best_model.pt # change this if resuming
  
  data:
    preprocessed_path: "data/processed/legit_processed.pt"
    window: 10
    stride: 5
    num_bins: 100
    group_by: "User"
    include_all_fraud: false
    padding_idx: 0
    mask_idx: 1
    unk_idx: 2
    ignore_idx: -100

metrics:
  run_name: pretrain
  wandb: true
  wandb_project: "feature-predictor"
  seed: 42
  deterministic: false
  log_gradients: false
  log_parameters: false
