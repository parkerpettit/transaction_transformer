# Implementation TODO List

This document outlines what needs to be implemented to complete the Transaction Transformer project according to the project overview rules.

## âœ… Completed Components

### 1. Data Schema & Encoders
- âœ… `FieldSchema` class with proper attributes
- âœ… `CatEncoder` with special IDs (PAD=0, MASK=1, UNK=2, realâ‰¥3)
- âœ… `NumBinner` with quantile-based bins
- âœ… Schema utilities (`cat_idx`, `cont_idx`)

### 2. Datasets & Collators
- âœ… `TxnDataset` with sliding window functionality
- âœ… `MLMTabCollator` with proper masking strategy
- âœ… `ARTabCollator` with shifted labels logic
- âœ… Proper dataset output format

### 3. Input Encoding & Embeddings
- âœ… `FrequencyEncoder` for continuous fields (NeRF-style)
- âœ… `EmbeddingLayer` with categorical embeddings and continuous projections
- âœ… Proper NaN handling for masked continuous values
- âœ… Device safety with `.to(self.freqs)`

### 4. Model Architecture
- âœ… `FieldTransformer` for intra-row interactions
- âœ… `RowProjector` for flattening field embeddings
- âœ… `SequenceTransformer` with auto-expanding PE and causal masks
- âœ… `RowExpander` for projecting back to per-field
- âœ… `TransactionPredictionHead` with per-field heads

### 5. Training Framework
- âœ… `BaseTrainer` with main training loop
- âœ… `MLMTrainer` with bidirectional attention
- âœ… `AutoregressiveTrainer` with causal attention
- âœ… Custom loss functions skeleton
- âœ… Wandb integration

## ğŸ”„ Partially Implemented Components

### 1. Custom Cross-Entropy Loss
- âœ… Skeleton implementation with proper structure
- âŒ **TODO**: Implement actual categorical label smoothing logic
- âŒ **TODO**: Implement actual numeric neighborhood smoothing logic
- âŒ **TODO**: Test with real data

### 2. Label Smoothing Strategies
- âœ… `CategoricalLabelSmoothing` class skeleton
- âœ… `NumericNeighborhoodSmoothing` class skeleton
- âŒ **TODO**: Integrate with custom loss function
- âŒ **TODO**: Test smoothing strategies

### 3. Training Scripts
- âœ… `pretrain.py` skeleton with proper structure
- âŒ **TODO**: Load actual schema and data
- âŒ **TODO**: Test end-to-end training

## âŒ Missing Components

### 1. Configuration Management
- âŒ **TODO**: Create proper config files (YAML) for different training modes
- âŒ **TODO**: CLI interface for overriding config settings
- âŒ **TODO**: Config validation and schema integration

### 2. Data Loading & Preprocessing
- âŒ **TODO**: Complete data preprocessing pipeline
- âŒ **TODO**: Save/load schema properly
- âŒ **TODO**: Handle data loading errors gracefully

### 3. Model Checkpointing
- âŒ **TODO**: Implement proper checkpoint saving/loading
- âŒ **TODO**: Save model config and schema with checkpoints
- âŒ **TODO**: Resume training from checkpoints

### 4. Evaluation & Metrics
- âŒ **TODO**: Implement per-field accuracy metrics
- âŒ **TODO**: Implement masked accuracy for MLM
- âŒ **TODO**: Implement bin occupancy histograms
- âŒ **TODO**: Implement F1 score for fraud detection

### 5. Inference Patterns
- âŒ **TODO**: MLM inference for imputation/scoring
- âŒ **TODO**: AR next-row prediction
- âŒ **TODO**: Fast path for last-step inference

### 6. Fraud Detection Model
- âŒ **TODO**: Complete `FraudDetectionModel` implementation
- âŒ **TODO**: Load pretrained embeddings
- âŒ **TODO**: Implement fraud classification head

### 7. Testing & Validation
- âŒ **TODO**: Unit tests for all components
- âŒ **TODO**: Integration tests for training pipeline
- âŒ **TODO**: Validation on real transaction data

## ğŸš¨ Critical Issues to Fix

### 1. Type Annotations
- âŒ **TODO**: Fix linter errors in trainers (model.config access)
- âŒ **TODO**: Add proper type hints throughout codebase
- âŒ **TODO**: Fix import issues

### 2. Data Integration
- âŒ **TODO**: Connect preprocessing output to training pipeline
- âŒ **TODO**: Ensure schema is properly passed through all components
- âŒ **TODO**: Test with actual transaction data

### 3. Loss Function Implementation
- âŒ **TODO**: Complete the custom loss function implementation
- âŒ **TODO**: Test loss computation with real data
- âŒ **TODO**: Verify label smoothing works correctly

## ğŸ“‹ Implementation Priority

### High Priority (Core Functionality)
1. **Complete custom loss function implementation**
2. **Fix data loading and schema integration**
3. **Test end-to-end training pipeline**
4. **Implement proper checkpointing**

### Medium Priority (Features)
1. **Add evaluation metrics**
2. **Complete fraud detection model**
3. **Add inference patterns**
4. **Implement configuration management**

### Low Priority (Polish)
1. **Add comprehensive testing**
2. **Optimize performance**
3. **Add documentation**
4. **Add CLI interface**

## ğŸ”§ Technical Debt

1. **Type Safety**: Many components need better type annotations
2. **Error Handling**: Add proper error handling throughout pipeline
3. **Logging**: Implement comprehensive logging system
4. **Performance**: Optimize data loading and model inference
5. **Memory**: Handle large datasets efficiently

## ğŸ“ Notes for Implementation

1. **Follow the project outline rules strictly** - especially the special IDs (PAD=0, MASK=1, UNK=2, realâ‰¥3)
2. **Use FieldSchema as single source of truth** - no hardcoded IDs or vocab sizes
3. **Implement proper device management** - avoid `torch.as_tensor` on CUDA tensors
4. **Test with small datasets first** - ensure pipeline works before scaling up
5. **Use wandb for all logging** - track experiments properly
6. **Save checkpoints frequently** - include schema and config in checkpoints

## ğŸ¯ Success Criteria

The implementation will be considered complete when:

1. âœ… MLM pretraining works end-to-end with real data
2. âœ… AR pretraining works end-to-end with real data
3. âœ… Custom loss function with label smoothing works correctly
4. âœ… Model can be saved/loaded with checkpoints
5. âœ… Fraud detection model can be trained on pretrained embeddings
6. âœ… All components have proper error handling and logging
7. âœ… Code passes all tests and linting checks 