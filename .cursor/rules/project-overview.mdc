---
alwaysApply: true
---

# Transaction Transformer Project Overview

## Project Goal

Recreate the UniTTab architecture with two interchangeable pretraining modes:
- **MLM (BERT-style)**: Masked token modeling on tabular time series
- **AR (Autoregressive)**: Next-row prediction

**Downstream Task**: Fraud detection on Transaction (card) dataset (single row type), with future-proofing for multi-row-type datasets.

**Core Innovation**: Uniform training for heterogeneous features. Numerical inputs use frequency encoding (no discretization), but targets are discrete bins. Unified cross-entropy loss with label smoothing strategies (standard label smoothing for categorical features and neighborhood label smoothing for continuous features).

## 1. Data Schema & Encoders

### FieldSchema Object (Single Source of Truth)
Create a `FieldSchema` object passed throughout the pipeline (dataset, collators, model, trainer):

```python
class FieldSchema:
    """Schema defining field configurations for preprocessing."""
    cat_features: list[str]          # names in order
    cont_features: list[str]
    cat_encoders: dict[str, CatEncoder]
    cont_binners: dict[str, NumBinner]
    time_cat: list[str] = field(default_factory=list)   # timestamp cat field names
    scaler: StandardScaler = field(default_factory=StandardScaler) # scaler for continuous features
    
    # Utilities:
    def cat_idx(self, name: str) -> int: 
        return self.cat_features.index(name)
    
    def cont_idx(self, name: str) -> int: 
        return self.cont_features.index(name)
```

### Categorical Encoders
- **Special IDs**: [PAD]=0, [MASK]=1, [UNK]=2, real IDs start at 3. UNK is reserved for values never seen in training data.


### Continuous Binners
- **Quantile-based bins** with ~100 bins per numeric feature (this should be adjustable by config or cli)
- **Build on train split only** (after normalization)
- **Save schema** 

### Important Rules
- **No hardcoded IDs or vocab sizes** in model/loader
- **Always consult schema** for sizes and IDs
- **Fit on train only** to avoid data leakage

## 2. Datasets & Collators

### Dataset Output Format
```python
{
    "cat": (L, C) long,      # Categorical features
    "cont": (L, F) float,    # Continuous features  
    "label": ... optional    # Downstream labels, ie for fraud
}
```
*Note: Eventually we will likely pad rows to common L in collator. Not needed for now.*

### MLM Collator (Pretraining)

#### Masking Strategy
- **Field masking**: p_field = 0.15 per cell
- **Row masking**: p_row = 0.10 per row
- **Joint timestamp masking**: One Bernoulli per (B,L) applied to all time subfields
- **Don't mask special ID tokens** Only mask non-special tokens.
#### Label Construction
- **Categorical**: `labels = original_ids` where masked, else `-100`
- **Continuous**: `labels = bin_ids` where masked, else `-100`

#### Input Masking
- **Categorical**: Set masked cells to `enc.mask_id` (==1)
- **Continuous**: Set masked cells to `NaN` (embedder treats as mask sentinel)

### AR Collator (Pretraining)

#### No Training Masks
- Inputs contain true sequence values
- No masking during training

#### Shifted Labels
- **Predict row t+1 from context ≤ t**
- `labels[:, :-1] = raw[:, 1:]` and `labels[:, -1] = -100`
- Set `labels = -100` where rows are padded

### General Rules
- **Always return torch tensors** (no NumPy in collators)
- **Handle padding consistently** across all collators

## 3. Input Encoding & Embeddings

### Categorical Fields
```python
nn.Embedding(vocab_size, D, padding_idx=0)
```
- **IDs**: 0=PAD, 1=MASK, 2=UNK, real≥3
- **Mask vector**: Embedding for id=1 becomes learned mask vector

### Continuous Fields

#### Normalization
- **Upstream normalization** (e.g., StandardScaler fit on train)
- **Use normalized values** for both frequency encoding and binning

#### Frequency Encoding (NeRF-style)
For value v, compute γ(v) = [sin(2ⁱπv), cos(2ⁱπv)]ᵢ₌₀ᴸ⁻¹ with L=8

**Implementation**:
- Vectorized encoder accepting (B, L, F) → (B, L, F, 2L)
- Per-feature linear projections -> (B, L, D)

#### Continuous Masking
1. Detect NaN values
2. Zero-fill before sin/cos
3. After projection, overwrite masked positions with global learned mask vector `nn.Parameter(D)`

#### Device Safety
- **Always use**: `v = v.to(self.freqs)`
- **Never use**: `torch.as_tensor(v)` on CUDA tensors

## 4. Model Architecture (UniTTab Core)

### Field Transformer
- After raw inputs are mapped to embeddings, they are fed into the Field Transformer.


### RowProjector Wₕ (Input Interface)
```python
# Flatten per-field embeddings
(B, L, K, D) → (B·L, K·D) → (B, L, M)
```
- M is embedding dimension of Sequence Transformer
- This step is necessary to standardize row types and reshape for Sequence Transformer.
- One `nn.Linear(K·D → M)` per row type (h)
- For Transaction dataset: 1 row type

### Sequence Transformer
```python
nn.TransformerEncoder with:
- Auto-expanding sinusoidal positional encoding (no fixed max_len)
- Auto-expanding cached causal mask (upper triangular; boolean True = masked)
- forward(x, causal: bool, key_padding_mask: BoolTensor|None)
```

**Modes**:
- **MLM**: `causal=False` (bidirectional)
- **AR**: `causal=True`


### RowExpander Sₕ (Output Interface)
```python
# Project row embeddings back to per-field
(B, L, M) → (B, L, K·D) → reshape (B, L, K, D)
```
- One `nn.Linear(M → K·D)` per row type (h)

### Per-Field Heads
- **One tiny classifier per field** (Linear or MLP: Linear-GELU-Linear)
- **Categorical field f**: output size V_f = vocab_size(field)
- **Continuous field g**: output size V_g = num_bins(field) (≈100)
- **Return**: dict {field_name: logits (B, L, V_f)} (do not stack—sizes differ)

## 5. Losses & Label Smoothing

### Custom Cross-Entropy Loss
- **Use**: Will implement custom NLLLoss function with `ignore_index = -100` for all fields
- **MLM**: Compute on masked positions only
- **AR**: Compute on shifted valid positions

### Label Smoothing Strategies

#### Categorical Smoothing
- **Exclude specials** if implementing custom smoothing

#### Numeric Neighborhood Smoothing
- **For true bin b**: Assign most mass to b, spread small ε over neighborhood b±5
- **Outside range**: Gets 0
- **Implementation**: Custom criterion with soft targets 

### Loss Computation
- **Sum of per-field losses** (optionally average by number of masked/valid positions)

## 6. Training Framework

### Trainer Class


### Features
- **AMP, grad accumulation, gradient clipping**
- **Optional LR scheduler**
- **Checkpoint save/load**
- **Expect model output**: {name: logits (B,L,V_f)}

### MLM Training Path
1. **Inputs**: Masked tensors from MLM collator
2. **Labels**: labels_cat, labels_cont with -100 elsewhere
3. **Transformer**: causal=False
4. **Loss**: CE only where labels ≠ -100

### AR Training Path
1. **Inputs**: Unmasked tensors from AR collator
2. **Labels**: labels_cat_ar, labels_cont_ar shifted left (last step -100)
3. **Transformer**: causal=True
4. **Prefer**: All-steps supervision (predict next row for every step)

### Logging
- **Track**: Total loss, per-feature accuracy
- **Save checkpoints** every time validation loss and/or f1 improves

## 7. Inference Patterns

### MLM Inference
- **Input**: Partially masked sequence
- **Run**: causal=False
- **Output**: Logits at masked cells for imputation/scoring

### AR Next-Row Prediction
- **Input**: Last L-1 rows
- **Run**: causal=True
- **Output**: Logits at time t=L for each field (argmax/sample)
- **Fast path**: Apply Sₕ+heads to last step only

## 8. Default Hyperparameters

### Special IDs
- **PAD=0, MASK=1, UNK=2, real≥3** (enforce everywhere)

### Frequency Encoding
- **L=8** (→ 16-dim per numeric)
- **Per-feature projections** to D

### Dimensions
- **D ≈ 72**
- **M (seq transformer d_model) ≈ 512-1024**

### Transformer
- **~12 layers, 8–12 heads**
- **FFN multiplier 4, dropout 0.1**
- **norm_first=True**

### Masking
- **p_field=0.15, p_row=0.10**
- **Timestamp joint masking** on

### Binning
- **~100 bins per numeric** (quantile bins)
- **Save bin edges**: edges[0]=−inf, edges[-1]=+inf

### Smoothing
- **Categorical**: 0.1
- **Numeric neighborhood**: ε_total≈0.1 across ±5 bins

## 9. Performance & Correctness Rules

### Computational Efficiency
- **Don't precompute** sin/cos offline for values
- **Vectorize**
- **Avoid Python loops** in hot paths

### Output Format
- **Return dict of logits**; do not stack across fields (different output sizes)

### Padding Handling
- **Always pass key_padding_mask** to SequenceTransformer once we add padding (not needed now)
- **Set labels at pads to -100**

### Device Management
- **Never convert CUDA tensors** via torch.as_tensor; use .to(other_tensor)
- **Registered buffers** (freqs, PE, masks) must auto-move with .to(device)

### Data Types
- **No NumPy in model/collate runtime**: Keep everything Torch-native

### Schema Compliance
- **No hardcoded ids/sizes**: Always read from schema
- **Row types**: Current dataset has one row type, but keep Wₕ and Sₕ structured for future multi-row support

### Training Strategy
- **AR training**: Prefer all-timesteps loss (shifted labels), not just last step
- **For last-only inference**: Add fast path downstream

### Monitoring
- **Track per-field masked accuracy** (MLM)
- **Track per-field loss, masked rate**
- **Track bin occupancy histograms** (catches silent bugs like empty bins)

### Reproducibility
- **Set seeds** for torch, numpy, Python. seed = 42 always.
- **Record all hyperparams and schema hashes** in checkpoints
- **Log everything to Weights&Biases** Use wandb for all logging. We have lots of free storage available, don't be afraid to use it. 

## 10. Common Pitfalls (Avoid These)

### Data Type Issues
- **Mixing NumPy and Torch masks** in collators → type errors
- **Use encoder's Torch helpers** for "is special" and random real IDs

### Masking Errors
- **Never mask specials**: PAD/MASK/UNK
- **Collator must check and skip** special tokens

### NaN Handling
- **Feeding NaNs through sin/cos**: Always zero-fill masked continuous values before frequency encoding
- **Overwrite with mask_token** after projection

### Output Format
- **Stacking logits across fields**: Output sizes differ; return dict keyed by field name

### Positional Encoding
- **Fixed max_len positional encoding**: Use auto-expanding PE and auto-expanding causal mask caches

### Label Smoothing
- **PyTorch's CE label_smoothing is uniform**: For numeric neighborhood smoothing, implement custom target builder/criterion

## 11. Minimal End-to-End Forward Pass

### MLM Path
1. **Collator**: Write [MASK] ids for cats, NaN for cont; create labels_* with -100 elsewhere
2. **EmbeddingLayer**: Cats via embeddings (id=1 is learned mask); cont via frequency→per-feature linear; NaN → mask vector
3. **Field Transformer**: Allows model to learn intra-row interactions.
4. **RowProjector Wₕ**: (B,L,K,D) → (B,L,M)
5. **SequenceTransformer**: Add PE; bidirectional. Allows model to learn inter-row interactions.
6. **RowExpander Sₕ**: (B,L,M) → (B,L,K,D)
7. **Per-field heads**: dict {name: logits (B,L,V_f)}
8. **Loss**: Custom CE with ignore_index=-100; categorical smoothing and numeric neighbor smoothing 

### AR Path
1. **Collator**: No masking; simply returns stacked tensors
2. **EmbeddingLayer**: Same as MLM but no NaN handling
3. **Field Transformer**: Allows model to learn intra-row interactions.
4. **RowProjector Wₕ**: Same as MLM
5. **SequenceTransformer**: Add PE; causal; include key_padding_mask
6. **RowExpander Sₕ**: Same as MLM
7. **Per-field heads**: Same as MLM
8. **Loss**: Slice off last transaction from collator as target. CE with ignore_index=-100 on shifted positions
# Transaction Transformer Project Overview

## Project Goal

Recreate the UniTTab architecture with two interchangeable pretraining modes:
- **MLM (BERT-style)**: Masked token modeling on tabular time series
- **AR (Autoregressive)**: Next-row prediction

**Downstream Task**: Fraud detection on Transaction (card) dataset (single row type), with future-proofing for multi-row-type datasets.

**Core Innovation**: Uniform training for heterogeneous features. Numerical inputs use frequency encoding (no discretization), but targets are discrete bins. Unified cross-entropy loss with label smoothing strategies (standard label smoothing for categorical features and neighborhood label smoothing for continuous features).

## 1. Data Schema & Encoders

### FieldSchema Object (Single Source of Truth)
Create a `FieldSchema` object passed throughout the pipeline (dataset, collators, model, trainer):

```python
class FieldSchema:
    """Schema defining field configurations for preprocessing."""
    cat_features: list[str]          # names in order
    cont_features: list[str]
    cat_encoders: dict[str, CatEncoder]
    cont_binners: dict[str, NumBinner]
    time_cat: list[str] = field(default_factory=list)   # timestamp cat field names
    scaler: StandardScaler = field(default_factory=StandardScaler) # scaler for continuous features
    
    # Utilities:
    def cat_idx(self, name: str) -> int: 
        return self.cat_features.index(name)
    
    def cont_idx(self, name: str) -> int: 
        return self.cont_features.index(name)
```

### Categorical Encoders
- **Special IDs**: [PAD]=0, [MASK]=1, [UNK]=2, real IDs start at 3. UNK is reserved for values never seen in training data.


### Continuous Binners
- **Quantile-based bins** with ~100 bins per numeric feature (this should be adjustable by config or cli)
- **Build on train split only** (after normalization)
- **Save schema** 

### Important Rules
- **No hardcoded IDs or vocab sizes** in model/loader
- **Always consult schema** for sizes and IDs
- **Fit on train only** to avoid data leakage

## 2. Datasets & Collators

### Dataset Output Format
```python
{
    "cat": (L, C) long,      # Categorical features
    "cont": (L, F) float,    # Continuous features  
    "label": ... optional    # Downstream labels, ie for fraud
}
```
*Note: Eventually we will likely pad rows to common L in collator. Not needed for now.*

### MLM Collator (Pretraining)

#### Masking Strategy
- **Field masking**: p_field = 0.15 per cell
- **Row masking**: p_row = 0.10 per row
- **Joint timestamp masking**: One Bernoulli per (B,L) applied to all time subfields
- **Don't mask special ID tokens** Only mask non-special tokens.
#### Label Construction
- **Categorical**: `labels = original_ids` where masked, else `-100`
- **Continuous**: `labels = bin_ids` where masked, else `-100`

#### Input Masking
- **Categorical**: Set masked cells to `enc.mask_id` (==1)
- **Continuous**: Set masked cells to `NaN` (embedder treats as mask sentinel)

### AR Collator (Pretraining)

#### No Training Masks
- Inputs contain true sequence values
- Causal masking during training


### General Rules
- **Always return torch tensors** (no NumPy in collators)
- **Handle padding consistently** across all collators

## 3. Input Encoding & Embeddings

### Categorical Fields
```python
nn.Embedding(vocab_size, D, padding_idx=0)
```
- **IDs**: 0=PAD, 1=MASK, 2=UNK, real≥3
- **Mask vector**: Embedding for id=1 becomes learned mask vector

### Continuous Fields

#### Normalization
- **Upstream normalization** (e.g., StandardScaler fit on train)
- **Use normalized values** for both frequency encoding and binning

#### Frequency Encoding (NeRF-style)
For value v, compute γ(v) = [sin(2ⁱπv), cos(2ⁱπv)]ᵢ₌₀ᴸ⁻¹ with L=8

**Implementation**:
- Vectorized encoder accepting (B, L, F) → (B, L, F, 2L)
- Per-feature linear projections -> (B, L, D)

#### Continuous Masking
1. Detect NaN values
2. Zero-fill before sin/cos
3. After projection, overwrite masked positions with global learned mask vector `nn.Parameter(D)`

#### Device Safety
- **Always use**: `v = v.to(self.freqs)`
- **Never use**: `torch.as_tensor(v)` on CUDA tensors

## 4. Model Architecture (UniTTab Core)

### Field Transformer
- After raw inputs are mapped to embeddings, they are fed into the Field Transformer.


### RowProjector Wₕ (Input Interface)
```python
# Flatten per-field embeddings
(B, L, K, D) → (B·L, K·D) → (B, L, M)
```
- M is embedding dimension of Sequence Transformer
- This step is necessary to standardize row types and reshape for Sequence Transformer.
- One `nn.Linear(K·D → M)` per row type (h)
- For Transaction dataset: 1 row type

### Sequence Transformer
```python
nn.TransformerEncoder with:
- Auto-expanding sinusoidal positional encoding (no fixed max_len)
- Auto-expanding cached causal mask (upper triangular; boolean True = masked)
- forward(x, causal: bool, key_padding_mask: BoolTensor|None)
```

**Modes**:
- **MLM**: `causal=False` (bidirectional)
- **AR**: `causal=True`


### RowExpander Sₕ (Output Interface)
```python
# Project row embeddings back to per-field
(B, L, M) → (B, L, K·D) → reshape (B, L, K, D)
```
- One `nn.Linear(M → K·D)` per row type (h)

### Per-Field Heads
- **One tiny classifier per field** (Linear or MLP: Linear-GELU-Linear)
- **Categorical field f**: output size V_f = vocab_size(field)
- **Continuous field g**: output size V_g = num_bins(field) (≈100)
- **Return**: dict {field_name: logits (B, L, V_f)} (do not stack—sizes differ)

## 5. Losses & Label Smoothing

### Custom Cross-Entropy Loss
- **Use**: Will implement custom NLLLoss function with `ignore_index = -100` for all fields
- **MLM**: Compute on masked positions only
- **AR**: Compute on shifted valid positions

### Label Smoothing Strategies

#### Categorical Smoothing
- **Exclude specials** if implementing custom smoothing

#### Numeric Neighborhood Smoothing
- **For true bin b**: Assign most mass to b, spread small ε over neighborhood b±5
- **Outside range**: Gets 0
- **Implementation**: Custom criterion with soft targets 

### Loss Computation
- **Sum of per-field losses** (optionally average by number of masked/valid positions)

## 6. Training Framework

### Trainer Class


### Features
- **AMP, grad accumulation, gradient clipping**
- **Optional LR scheduler**
- **Checkpoint save/load**
- **Expect model output**: {name: logits (B,L,V_f)}

### MLM Training Path
1. **Inputs**: Masked tensors from MLM collator
2. **Labels**: labels_cat, labels_cont with -100 elsewhere
3. **Transformer**: causal=False
4. **Loss**: CE only where labels ≠ -100

### AR Training Path
1. **Inputs**: Unmasked tensors from AR collator
2. **Labels**: labels_cat_ar, labels_cont_ar shifted left (last step -100)
3. **Transformer**: causal=True
4. **Prefer**: All-steps supervision (predict next row for every step)

### Logging
- **Track**: Total loss, per-feature accuracy
- **Save checkpoints** every time validation loss and/or f1 improves

## 7. Inference Patterns

### MLM Inference
- **Input**: Partially masked sequence
- **Run**: causal=False
- **Output**: Logits at masked cells for imputation/scoring

### AR Next-Row Prediction
- **Input**: Last L-1 rows
- **Run**: causal=True
- **Output**: Logits at time t=L for each field (argmax/sample)
- **Fast path**: Apply Sₕ+heads to last step only

## 8. Default Hyperparameters

### Special IDs
- **PAD=0, MASK=1, UNK=2, real≥3** (enforce everywhere)

### Frequency Encoding
- **L=8** (→ 16-dim per numeric)
- **Per-feature projections** to D

### Dimensions
- **D ≈ 72**
- **M (seq transformer d_model) ≈ 512-1024**

### Transformer
- **~12 layers, 8–12 heads**
- **FFN multiplier 4, dropout 0.1**
- **norm_first=True**

### Masking
- **p_field=0.15, p_row=0.10**
- **Timestamp joint masking** on

### Binning
- **~100 bins per numeric** (quantile bins)
- **Save bin edges**: edges[0]=−inf, edges[-1]=+inf

### Smoothing
- **Categorical**: 0.1
- **Numeric neighborhood**: ε_total≈0.1 across ±5 bins

## 9. Performance & Correctness Rules

### Computational Efficiency
- **Don't precompute** sin/cos offline for values
- **Vectorize**
- **Avoid Python loops** in hot paths

### Output Format
- **Return dict of logits**; do not stack across fields (different output sizes)

### Padding Handling
- **Always pass key_padding_mask** to SequenceTransformer once we add padding (not needed now)
- **Set labels at pads to -100**

### Device Management
- **Never convert CUDA tensors** via torch.as_tensor; use .to(other_tensor)
- **Registered buffers** (freqs, PE, masks) must auto-move with .to(device)

### Data Types
- **No NumPy in model/collate runtime**: Keep everything Torch-native

### Schema Compliance
- **No hardcoded ids/sizes**: Always read from schema
- **Row types**: Current dataset has one row type, but keep Wₕ and Sₕ structured for future multi-row support

### Training Strategy
- **AR training**: Prefer all-timesteps loss (shifted labels), not just last step
- **For last-only inference**: Add fast path downstream

### Monitoring
- **Track per-field masked accuracy** (MLM)
- **Track per-field loss, masked rate**
- **Track bin occupancy histograms** (catches silent bugs like empty bins)

### Reproducibility
- **Set seeds** for torch, numpy, Python. seed = 42 always.
- **Record all hyperparams and schema hashes** in checkpoints
- **Log everything to Weights&Biases** Use wandb for all logging. We have lots of free storage available, don't be afraid to use it. 

## 10. Common Pitfalls (Avoid These)

### Data Type Issues
- **Mixing NumPy and Torch masks** in collators → type errors
- **Use encoder's Torch helpers** for "is special" and random real IDs

### Masking Errors
- **Never mask specials**: PAD/MASK/UNK
- **Collator must check and skip** special tokens

### NaN Handling
- **Feeding NaNs through sin/cos**: Always zero-fill masked continuous values before frequency encoding
- **Overwrite with mask_token** after projection

### Output Format
- **Stacking logits across fields**: Output sizes differ; return dict keyed by field name

### Positional Encoding
- **Fixed max_len positional encoding**: Use auto-expanding PE and auto-expanding causal mask caches

### Label Smoothing
- **PyTorch's CE label_smoothing is uniform**: For numeric neighborhood smoothing, implement custom target builder/criterion

## 11. Minimal End-to-End Forward Pass

### MLM Path
1. **Collator**: Write [MASK] ids for cats, NaN for cont; create labels_* with -100 elsewhere
2. **EmbeddingLayer**: Cats via embeddings (id=1 is learned mask); cont via frequency→per-feature linear; NaN → mask vector
3. **Field Transformer**: Allows model to learn intra-row interactions.
4. **RowProjector Wₕ**: (B,L,K,D) → (B,L,M)
5. **SequenceTransformer**: Add PE; bidirectional. Allows model to learn inter-row interactions.
6. **RowExpander Sₕ**: (B,L,M) → (B,L,K,D)
7. **Per-field heads**: dict {name: logits (B,L,V_f)}
8. **Loss**: Custom CE with ignore_index=-100; categorical smoothing and numeric neighbor smoothing 

### AR Path
1. **Collator**: No masking; simply returns stacked tensors
2. **EmbeddingLayer**: Same as MLM but no NaN handling
3. **Field Transformer**: Allows model to learn intra-row interactions.
4. **RowProjector Wₕ**: Same as MLM
5. **SequenceTransformer**: Add PE; causal; include key_padding_mask
6. **RowExpander Sₕ**: Same as MLM
7. **Per-field heads**: Same as MLM
8. **Loss**: Slice off last transaction from collator as target. CE with ignore_index=-100 on shifted positions
